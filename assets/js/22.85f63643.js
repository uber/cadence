(window.webpackJsonp=window.webpackJsonp||[]).push([[22],{373:function(t,e,r){"use strict";r.r(e);var a=r(8),i=Object(a.a)({},(function(){var t=this,e=t.$createElement,r=t._self._c||e;return r("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[r("h1",{attrs:{id:"activities"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#activities"}},[t._v("#")]),t._v(" Activities")]),t._v(" "),r("p",[t._v("Fault-oblivious stateful "),r("Term",{attrs:{term:"workflow"}}),t._v(" code is the core abstraction of Cadence. But, due to deterministic execution requirements, they are not allowed to call any external API directly.\nInstead they orchestrate execution of "),r("Term",{attrs:{term:"activity",show:"activities"}}),t._v(". In its simplest form, a Cadence "),r("Term",{attrs:{term:"activity"}}),t._v(" is a function or an object method in one of the supported languages.\nCadence does not recover "),r("Term",{attrs:{term:"activity"}}),t._v(" state in case of failures. Therefore an "),r("Term",{attrs:{term:"activity"}}),t._v(" function is allowed to contain any code without restrictions.")],1),t._v(" "),r("p",[r("Term",{attrs:{term:"activity",show:"Activities"}}),t._v(" are invoked asynchronously though "),r("Term",{attrs:{term:"task_list",show:"task_lists"}}),t._v(". A "),r("Term",{attrs:{term:"task_list"}}),t._v(" is essentially a queue used to store an "),r("Term",{attrs:{term:"activity_task"}}),t._v(" until it is picked up by an available "),r("Term",{attrs:{term:"worker"}}),t._v(". The "),r("Term",{attrs:{term:"worker"}}),t._v(" processes an "),r("Term",{attrs:{term:"activity"}}),t._v(" by invoking its implementation function. When the function returns, the "),r("Term",{attrs:{term:"worker"}}),t._v(" reports the result back to the Cadence service which in turn notifies the "),r("Term",{attrs:{term:"workflow"}}),t._v(" about completion. It is possible to implement an "),r("Term",{attrs:{term:"activity"}}),t._v(" fully asynchronously by completing it from a different process.")],1),t._v(" "),r("h2",{attrs:{id:"timeouts"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#timeouts"}},[t._v("#")]),t._v(" Timeouts")]),t._v(" "),r("p",[t._v("Cadence does not impose any system limit on "),r("Term",{attrs:{term:"activity"}}),t._v(" duration. It is up to the application to choose the timeouts for its execution. These are the configurable "),r("Term",{attrs:{term:"activity"}}),t._v(" timeouts:")],1),t._v(" "),r("ul",[r("li",[r("code",[t._v("ScheduleToStart")]),t._v(" is the maximum time from a "),r("Term",{attrs:{term:"workflow"}}),t._v(" requesting "),r("Term",{attrs:{term:"activity"}}),t._v(" execution to a "),r("Term",{attrs:{term:"worker"}}),t._v(" starting its execution. The usual reason for this timeout to fire is all "),r("Term",{attrs:{term:"worker",show:"workers"}}),t._v(" being down or not being able to keep up with the request rate. We recommend setting this timeout to the maximum time a "),r("Term",{attrs:{term:"workflow"}}),t._v(" is willing to wait for an "),r("Term",{attrs:{term:"activity"}}),t._v(" execution in the presence of all possible "),r("Term",{attrs:{term:"worker"}}),t._v(" outages.")],1),t._v(" "),r("li",[r("code",[t._v("StartToClose")]),t._v(" is the maximum time an "),r("Term",{attrs:{term:"activity"}}),t._v(" can execute after it was picked by a "),r("Term",{attrs:{term:"worker"}}),t._v(".")],1),t._v(" "),r("li",[r("code",[t._v("ScheduleToClose")]),t._v(" is the maximum time from the "),r("Term",{attrs:{term:"workflow"}}),t._v(" requesting an "),r("Term",{attrs:{term:"activity"}}),t._v(" execution to its completion.")],1),t._v(" "),r("li",[r("code",[t._v("Heartbeat")]),t._v(" is the maximum time between heartbeat requests. See "),r("a",{attrs:{href:"#long-running-activities"}},[t._v("Long Running Activities")]),t._v(".")])]),t._v(" "),r("p",[t._v("Either "),r("code",[t._v("ScheduleToClose")]),t._v(" or both "),r("code",[t._v("ScheduleToStart")]),t._v(" and "),r("code",[t._v("StartToClose")]),t._v(" timeouts are required.")]),t._v(" "),r("h2",{attrs:{id:"retries"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#retries"}},[t._v("#")]),t._v(" Retries")]),t._v(" "),r("p",[t._v("As Cadence doesn't recover an "),r("Term",{attrs:{term:"activity"}}),t._v("'s state and they can communicate to any external system, failures are expected. Therefore, Cadence supports automatic "),r("Term",{attrs:{term:"activity"}}),t._v(" retries. Any "),r("Term",{attrs:{term:"activity"}}),t._v(" when invoked can have an associated retry policy. Here are the retry policy parameters:")],1),t._v(" "),r("ul",[r("li",[r("code",[t._v("InitialInterval")]),t._v(" is a delay before the first retry.")]),t._v(" "),r("li",[r("code",[t._v("BackoffCoefficient")]),t._v(". Retry policies are exponential. The coefficient specifies how fast the retry interval is growing. The coefficient of 1 means that the retry interval is always equal to the "),r("code",[t._v("InitialInterval")]),t._v(".")]),t._v(" "),r("li",[r("code",[t._v("MaximumInterval")]),t._v(" specifies the maximum interval between retries. Useful for coefficients more than 1.")]),t._v(" "),r("li",[r("code",[t._v("MaximumAttempts")]),t._v(" specifies how many times to attempt to execute an "),r("Term",{attrs:{term:"activity"}}),t._v(" in the presence of failures. If this limit is exceeded, the error is returned back to the "),r("Term",{attrs:{term:"workflow"}}),t._v(" that invoked the "),r("Term",{attrs:{term:"activity"}}),t._v(". Not required if "),r("code",[t._v("ExpirationInterval")]),t._v(" is specified.")],1),t._v(" "),r("li",[r("code",[t._v("ExpirationInterval")]),t._v(" specifies for how long to attempt executing an "),r("Term",{attrs:{term:"activity"}}),t._v(" in the presence of failures. If this interval is exceeded, the error is returned back to the "),r("Term",{attrs:{term:"workflow"}}),t._v(" that invoked the "),r("Term",{attrs:{term:"activity"}}),t._v(". Not required if "),r("code",[t._v("MaximumAttempts")]),t._v(" is specified.")],1),t._v(" "),r("li",[r("code",[t._v("NonRetryableErrorReasons")]),t._v(" allows you to specify errors that shouldn't be retried. For example retrying invalid arguments error doesn't make sense in some scenarios.")])]),t._v(" "),r("p",[t._v("There are scenarios when not a single "),r("Term",{attrs:{term:"activity"}}),t._v(" but rather the whole part of a "),r("Term",{attrs:{term:"workflow"}}),t._v(" should be retried on failure. For example, a media encoding "),r("Term",{attrs:{term:"workflow"}}),t._v(" that downloads a file to a host, processes it, and then uploads the result back to storage. In this "),r("Term",{attrs:{term:"workflow"}}),t._v(", if the host that hosts the "),r("Term",{attrs:{term:"worker"}}),t._v(" dies, all three "),r("Term",{attrs:{term:"activity",show:"activities"}}),t._v(" should be retried on a different host. Such retries should be handled by the "),r("Term",{attrs:{term:"workflow"}}),t._v(" code as they are very use case specific.")],1),t._v(" "),r("h2",{attrs:{id:"long-running-activities"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#long-running-activities"}},[t._v("#")]),t._v(" Long Running Activities")]),t._v(" "),r("p",[t._v("For long running "),r("Term",{attrs:{term:"activity",show:"activities"}}),t._v(", we recommended that you specify a relatively short heartbeat timeout and constantly heartbeat. This way "),r("Term",{attrs:{term:"worker"}}),t._v(" failures for even very long running "),r("Term",{attrs:{term:"activity",show:"activities"}}),t._v(" can be handled in a timely manner. An "),r("Term",{attrs:{term:"activity"}}),t._v(" that specifies the heartbeat timeout is expected to call the heartbeat method "),r("em",[t._v("periodically")]),t._v(" from its implementation.")],1),t._v(" "),r("p",[t._v("A heartbeat request can include application specific payload. This is useful to save "),r("Term",{attrs:{term:"activity"}}),t._v(" execution progress. If an "),r("Term",{attrs:{term:"activity"}}),t._v(" times out due to a missed heartbeat, the next attempt to execute it can access that progress and continue its execution from that point.")],1),t._v(" "),r("p",[t._v("Long running "),r("Term",{attrs:{term:"activity",show:"activities"}}),t._v(" can be used as a special case of leader election. Cadence timeouts use second resolution. So it is not a solution for realtime applications. But if it is okay to react to the process failure within a few seconds, then a Cadence heartbeat "),r("Term",{attrs:{term:"activity"}}),t._v(" is a good fit.")],1),t._v(" "),r("p",[t._v("One common use case for such leader election is monitoring. An "),r("Term",{attrs:{term:"activity"}}),t._v(" executes an internal loop that periodically polls some API and checks for some condition. It also heartbeats on every iteration. If the condition is satisfied, the "),r("Term",{attrs:{term:"activity"}}),t._v(" completes which lets its "),r("Term",{attrs:{term:"workflow"}}),t._v(" to handle it. If the "),r("Term",{attrs:{term:"activity_worker"}}),t._v(" dies, the "),r("Term",{attrs:{term:"activity"}}),t._v(" times out after the heartbeat interval is exceeded and is retried on a different "),r("Term",{attrs:{term:"worker"}}),t._v(". The same pattern works for polling for new files in Amazon S3 buckets or responses in REST or other synchronous APIs.")],1),t._v(" "),r("h2",{attrs:{id:"cancellation"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#cancellation"}},[t._v("#")]),t._v(" Cancellation")]),t._v(" "),r("p",[t._v("A "),r("Term",{attrs:{term:"workflow"}}),t._v(" can request an "),r("Term",{attrs:{term:"activity"}}),t._v(" cancellation. Currently the only way for an "),r("Term",{attrs:{term:"activity"}}),t._v(" to learn that it was cancelled is through heart beating. The heartbeat request fails with a special error indicating that the "),r("Term",{attrs:{term:"activity"}}),t._v(" was cancelled. Then it is up to the "),r("Term",{attrs:{term:"activity"}}),t._v(" implementation to perform all the necessary cleanup and report that it is done with it. It is up to the "),r("Term",{attrs:{term:"workflow"}}),t._v(" implementation to decide if it wants to wait for the "),r("Term",{attrs:{term:"activity"}}),t._v(" cancellation confirmation or just proceed without waiting.")],1),t._v(" "),r("p",[t._v("Another common case for "),r("Term",{attrs:{term:"activity"}}),t._v(" heartbeat failure is that the "),r("Term",{attrs:{term:"workflow"}}),t._v(" that invoked it is in a completed state. In this case an "),r("Term",{attrs:{term:"activity"}}),t._v(" is expected to perform cleanup as well.")],1),t._v(" "),r("h2",{attrs:{id:"activity-task-routing-through-task-lists"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#activity-task-routing-through-task-lists"}},[t._v("#")]),t._v(" Activity Task Routing through Task Lists")]),t._v(" "),r("p",[r("Term",{attrs:{term:"activity",show:"Activities"}}),t._v(" are dispatched to "),r("Term",{attrs:{term:"worker",show:"workers"}}),t._v(" through "),r("Term",{attrs:{term:"task_list",show:"task_lists"}}),t._v(". "),r("Term",{attrs:{term:"task_list",show:"Task_lists"}}),t._v(" are queues that "),r("Term",{attrs:{term:"worker",show:"workers"}}),t._v(" listen on. "),r("Term",{attrs:{term:"task_list",show:"Task_lists"}}),t._v(" are highly dynamic and lightweight. They don't need to be explicitly registered. And it is okay to have one "),r("Term",{attrs:{term:"task_list"}}),t._v(" per "),r("Term",{attrs:{term:"worker"}}),t._v(" process. It is normal to have more than one "),r("Term",{attrs:{term:"activity"}}),t._v(" type to be invoked through a single "),r("Term",{attrs:{term:"task_list"}}),t._v(". And it is normal in some cases (like host routing) to invoke the same "),r("Term",{attrs:{term:"activity"}}),t._v(" type on multiple "),r("Term",{attrs:{term:"task_list",show:"task_lists"}}),t._v(".")],1),t._v(" "),r("p",[t._v("Here are some use cases for employing multiple "),r("Term",{attrs:{term:"activity_task_list",show:"activity_task_lists"}}),t._v(" in a single workflow:")],1),t._v(" "),r("ul",[r("li",[r("em",[t._v("Flow control")]),t._v(". A "),r("Term",{attrs:{term:"worker"}}),t._v(" that consumes from a "),r("Term",{attrs:{term:"task_list"}}),t._v(" asks for an "),r("Term",{attrs:{term:"activity_task"}}),t._v(" only when it has available capacity. So "),r("Term",{attrs:{term:"worker",show:"workers"}}),t._v(" are never overloaded by request spikes. If "),r("Term",{attrs:{term:"activity"}}),t._v(" executions are requested faster than "),r("Term",{attrs:{term:"worker",show:"workers"}}),t._v(" can process them, they are backlogged in the "),r("Term",{attrs:{term:"task_list"}}),t._v(".")],1),t._v(" "),r("li",[r("em",[t._v("Throttling")]),t._v(". Each "),r("Term",{attrs:{term:"activity_worker"}}),t._v(" can specify the maximum rate it is allowed to processes "),r("Term",{attrs:{term:"activity",show:"activities"}}),t._v(" on a "),r("Term",{attrs:{term:"task_list"}}),t._v(". It does not exceed this limit even if it has spare capacity. There is also support for global "),r("Term",{attrs:{term:"task_list"}}),t._v(" rate limiting. This limit works across all "),r("Term",{attrs:{term:"worker",show:"workers"}}),t._v(" for the given "),r("Term",{attrs:{term:"task_list"}}),t._v(". It is frequently used to limit load on a downstream service that an "),r("Term",{attrs:{term:"activity"}}),t._v(" calls into.")],1),t._v(" "),r("li",[r("em",[t._v("Deploying a set of "),r("Term",{attrs:{term:"activity",show:"activities"}}),t._v(" independently")],1),t._v(". Think about a service that hosts "),r("Term",{attrs:{term:"activity",show:"activities"}}),t._v(" and can be deployed independently from other "),r("Term",{attrs:{term:"activity",show:"activities"}}),t._v(" and "),r("Term",{attrs:{term:"workflow",show:"workflows"}}),t._v(". To send "),r("Term",{attrs:{term:"activity_task",show:"activity_tasks"}}),t._v(" to this service, a separate "),r("Term",{attrs:{term:"task_list"}}),t._v(" is needed.")],1),t._v(" "),r("li",[r("em",[r("Term",{attrs:{term:"worker",show:"Workers"}}),t._v(" with different capabilities")],1),t._v(". For example, "),r("Term",{attrs:{term:"worker",show:"workers"}}),t._v(" on GPU boxes vs non GPU boxes. Having two separate "),r("Term",{attrs:{term:"task_list",show:"task_lists"}}),t._v(" in this case allows "),r("Term",{attrs:{term:"workflow",show:"workflows"}}),t._v(" to pick which one to send "),r("Term",{attrs:{term:"activity"}}),t._v(" an execution request to.")],1),t._v(" "),r("li",[r("em",[t._v("Routing "),r("Term",{attrs:{term:"activity"}}),t._v(" to a specific host")],1),t._v(". For example, in the media encoding case the transform and upload "),r("Term",{attrs:{term:"activity"}}),t._v(" have to run on the same host as the download one.")],1),t._v(" "),r("li",[r("em",[t._v("Routing "),r("Term",{attrs:{term:"activity"}}),t._v(" to a specific process")],1),t._v(". For example, some "),r("Term",{attrs:{term:"activity",show:"activities"}}),t._v(" load large data sets and caches it in the process. The "),r("Term",{attrs:{term:"activity",show:"activities"}}),t._v(" that rely on this data set should be routed to the same process.")],1),t._v(" "),r("li",[r("em",[t._v("Multiple priorities")]),t._v(". One "),r("Term",{attrs:{term:"task_list"}}),t._v(" per priority and having a "),r("Term",{attrs:{term:"worker"}}),t._v(" pool per priority.")],1),t._v(" "),r("li",[r("em",[t._v("Versioning")]),t._v(". A new backwards incompatible implementation of an "),r("Term",{attrs:{term:"activity"}}),t._v(" might use a different "),r("Term",{attrs:{term:"task_list"}}),t._v(".")],1)]),t._v(" "),r("h2",{attrs:{id:"asynchronous-activity-completion"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#asynchronous-activity-completion"}},[t._v("#")]),t._v(" Asynchronous Activity Completion")]),t._v(" "),r("p",[t._v("By default an "),r("Term",{attrs:{term:"activity"}}),t._v(" is a function or a method depending on a client side library language. As soon as the function returns, an "),r("Term",{attrs:{term:"activity"}}),t._v(" completes. But in some cases an "),r("Term",{attrs:{term:"activity"}}),t._v(" implementation is asynchronous. For example it is forwarded to an external system through a message queue. And the reply comes through a different queue.")],1),t._v(" "),r("p",[t._v("To support such use cases, Cadence allows "),r("Term",{attrs:{term:"activity"}}),t._v(" implementations that do not complete upon "),r("Term",{attrs:{term:"activity"}}),t._v(" function completions. A separate API should be used in this case to complete the "),r("Term",{attrs:{term:"activity"}}),t._v(". This API can be called from any process, even in a different programming language, that the original "),r("Term",{attrs:{term:"activity_worker"}}),t._v(" used.")],1),t._v(" "),r("h2",{attrs:{id:"local-activities"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#local-activities"}},[t._v("#")]),t._v(" Local Activities")]),t._v(" "),r("p",[t._v("Some of the "),r("Term",{attrs:{term:"activity",show:"activities"}}),t._v(" are very short lived and do not need the queing semantic, flow control, rate limiting and routing capabilities. For these Cadence supports so called "),r("em",[r("Term",{attrs:{term:"local_activity"}})],1),t._v(" feature. "),r("Term",{attrs:{term:"local_activity",show:"Local_activities"}}),t._v(" are executed in the same "),r("Term",{attrs:{term:"worker"}}),t._v(" process as the "),r("Term",{attrs:{term:"workflow"}}),t._v(" that invoked them. Consider using "),r("Term",{attrs:{term:"local_activity",show:"local_activities"}}),t._v(" for functions that are:")],1),t._v(" "),r("ul",[r("li",[t._v("no longer than a few seconds")]),t._v(" "),r("li",[t._v("do not require global rate limiting")]),t._v(" "),r("li",[t._v("do not require routing to specific "),r("Term",{attrs:{term:"worker",show:"workers"}}),t._v(" or pools of "),r("Term",{attrs:{term:"worker",show:"workers"}})],1),t._v(" "),r("li",[t._v("can be implemented in the same binary as the "),r("Term",{attrs:{term:"workflow"}}),t._v(" that invokes them")],1)]),t._v(" "),r("p",[t._v("The main benefit of "),r("Term",{attrs:{term:"local_activity",show:"local_activities"}}),t._v(" is that they are much more efficient in utilizing Cadence service resources and have much lower latency overhead comparing to the usual "),r("Term",{attrs:{term:"activity"}}),t._v(" invocation.")],1)])}),[],!1,null,null,null);e.default=i.exports}}]);