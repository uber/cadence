persistence:
  defaultStore: cass-default
  visibilityStore: cass-visibility
  numHistoryShards: 4
  datastores:
    cass-default:
      nosql:
        pluginName: "cassandra"
        hosts: "127.0.0.1"
        keyspace: "cadence_cluster1"
    cass-visibility:
      nosql:
        pluginName: "cassandra"
        hosts: "127.0.0.1"
        keyspace: "cadence_visibility_cluster1"

ringpop:
  name: cadence_cluster1
  bootstrapMode: hosts
  bootstrapHosts: [ "127.0.0.1:8933", "127.0.0.1:8934", "127.0.0.1:8935", "127.0.0.1:8940" ]
  maxJoinDuration: 30s

services:
  frontend:
    rpc:
      port: 8933
      grpcPort: 8833
      bindOnLocalHost: true
    metrics:
      statsd:
        hostPort: "127.0.0.1:8125"
        prefix: "cadence_cluster1"
    pprof:
      port: 8936

  matching:
    rpc:
      port: 8935
      grpcPort: 8835
      bindOnLocalHost: true
    metrics:
      statsd:
        hostPort: "127.0.0.1:8125"
        prefix: "cadence_cluster1"
    pprof:
      port: 8938

  history:
    rpc:
      port: 8934
      grpcPort: 8834
      bindOnLocalHost: true
    metrics:
      statsd:
        hostPort: "127.0.0.1:8125"
        prefix: "cadence_cluster1"
    pprof:
      port: 8937

  worker:
    rpc:
      port: 8940
      bindOnLocalHost: true
    metrics:
      statsd:
        hostPort: "127.0.0.1:8125"
        prefix: "cadence_cluster1"
    pprof:
      port: 8941

  shard-manager:
    rpc:
      port: 7952
      bindOnLocalHost: true
    metrics:
      statsd:
        hostPort: "127.0.0.1:8125"
        prefix: "cadence"
    pprof:
      port: 7953

clusterGroupMetadata:
  failoverVersionIncrement: 10
  primaryClusterName: "cluster0"
  currentClusterName: "cluster1"
  clusterRedirectionPolicy:
    policy: "all-domain-apis-forwarding" # if network communication overhead between clusters is high, consider use "selected-apis-forwarding" instead, but workflow/activity workers need to be connected to each cluster to keep high availability
  clusterGroup:
    cluster0:
      enabled: true
      initialFailoverVersion: 1
      rpcName: "cadence-frontend"
      rpcAddress: "localhost:7833" # this is to let worker service and XDC replicator connected to the frontend service. In cluster setup, localhost will not work
      rpcTransport: "grpc"
    cluster1:
      enabled: true
      initialFailoverVersion: 0
      rpcName: "cadence-frontend"
      rpcAddress: "localhost:8833" # this is to let worker service and XDC replicator connected to the frontend service. In cluster setup, localhost will not work
      rpcTransport: "grpc"
    cluster2:
      enabled: true
      initialFailoverVersion: 2
      rpcName: "cadence-frontend"
      rpcAddress: "localhost:9833" # this is to let worker service and XDC replicator connected to the frontend service. In cluster setup, localhost will not work
      rpcTransport: "grpc"

archival:
  history:
    status: "enabled"
    enableRead: true
    provider:
      filestore:
        fileMode: "0666"
        dirMode: "0766"
  visibility:
    status: "enabled"
    enableRead: true
    provider:
      filestore:
        fileMode: "0666"
        dirMode: "0766"

domainDefaults:
  archival:
    history:
      status: "enabled"
      URI: "file:///tmp/cadence_archival/development"
    visibility:
      status: "enabled"
      URI: "file:///tmp/cadence_vis_archival/development"

blobstore:
  filestore:
    outputDirectory: "/tmp/blobstore"

dynamicconfig:
  client: filebased
  configstore:
    pollInterval: "10s"
    updateRetryAttempts: 2
    FetchTimeout: "2s"
    UpdateTimeout: "2s"
  filebased:
    filepath: "config/dynamicconfig/development.yaml"
    pollInterval: "10s"
