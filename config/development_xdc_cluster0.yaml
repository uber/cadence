persistence:
  defaultStore: cass-default
  visibilityStore: cass-visibility
  numHistoryShards: 4
  datastores:
    cass-default:
      nosql:
        pluginName: "cassandra"
        hosts: "127.0.0.1"
        keyspace: "cadence_cluster0"
    cass-visibility:
      nosql:
        pluginName: "cassandra"
        hosts: "127.0.0.1"
        keyspace: "cadence_visibility_cluster0"

ringpop:
  name: cadence_cluster0
  bootstrapMode: hosts
  bootstrapHosts: [ "127.0.0.1:7933", "127.0.0.1:7934", "127.0.0.1:7935", "127.0.0.1:7940" ]
  maxJoinDuration: 30s

services:
  frontend:
    rpc:
      port: 7933
      grpcPort: 7833
      bindOnLocalHost: true
    metrics:
      statsd:
        hostPort: "127.0.0.1:8125"
        prefix: "cadence_cluster0"
    pprof:
      port: 7936

  matching:
    rpc:
      port: 7935
      grpcPort: 7835
      bindOnLocalHost: true
    metrics:
      statsd:
        hostPort: "127.0.0.1:8125"
        prefix: "cadence_cluster0"
    pprof:
      port: 7938

  history:
    rpc:
      port: 7934
      grpcPort: 7834
      bindOnLocalHost: true
    metrics:
      statsd:
        hostPort: "127.0.0.1:8125"
        prefix: "cadence_cluster0"
    pprof:
      port: 7937

  worker:
    rpc:
      port: 7940
      bindOnLocalHost: true
    metrics:
      statsd:
        hostPort: "127.0.0.1:8125"
        prefix: "cadence_cluster0"
    pprof:
      port: 7941


  shard-manager:
    rpc:
      port: 7951
      bindOnLocalHost: true
    metrics:
      statsd:
        hostPort: "127.0.0.1:8125"
        prefix: "cadence"
    pprof:
      port: 7952


clusterGroupMetadata:
  failoverVersionIncrement: 10
  primaryClusterName: "cluster0"
  currentClusterName: "cluster0"
  clusterRedirectionPolicy:
    policy: "all-domain-apis-forwarding" # if network communication overhead between clusters is high, consider use "selected-apis-forwarding" instead, but workflow/activity workers need to be connected to each cluster to keep high availability
  clusterGroup:
    cluster0:
      enabled: true
      initialFailoverVersion: 1
      rpcName: "cadence-frontend"
      rpcAddress: "localhost:7833" # this is to let worker service and XDC replicator connected to the frontend service. In cluster setup, localhost will not work
      rpcTransport: "grpc"
    cluster1:
      enabled: true
      initialFailoverVersion: 0
      rpcName: "cadence-frontend"
      rpcAddress: "localhost:8833" # this is to let worker service and XDC replicator connected to the frontend service. In cluster setup, localhost will not work
      rpcTransport: "grpc"
    cluster2:
      enabled: true
      initialFailoverVersion: 2
      rpcName: "cadence-frontend"
      rpcAddress: "localhost:9833" # this is to let worker service and XDC replicator connected to the frontend service. In cluster setup, localhost will not work
      rpcTransport: "grpc"



archival:
  history:
    status: "enabled"
    enableRead: true
    provider:
      filestore:
        fileMode: "0666"
        dirMode: "0766"
  visibility:
    status: "enabled"
    enableRead: true
    provider:
      filestore:
        fileMode: "0666"
        dirMode: "0766"

domainDefaults:
  archival:
    history:
      status: "enabled"
      URI: "file:///tmp/cadence_archival/development"
    visibility:
      status: "enabled"
      URI: "file:///tmp/cadence_vis_archival/development"

blobstore:
  filestore:
    outputDirectory: "/tmp/blobstore"

dynamicconfig:
  client: filebased
  configstore:
    pollInterval: "10s"
    updateRetryAttempts: 2
    FetchTimeout: "2s"
    UpdateTimeout: "2s"
  filebased:
    filepath: "config/dynamicconfig/development.yaml"
    pollInterval: "10s"
